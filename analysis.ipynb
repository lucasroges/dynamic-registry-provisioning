{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import msgpack\n",
    "import edge_sim_py\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"logs\"\n",
    "\n",
    "seed = \"1\"\n",
    "\n",
    "last_step = 3600\n",
    "\n",
    "variations = [\n",
    "    \"nodes=100;unique_images=08\",\n",
    "    \"nodes=100;unique_images=32\",\n",
    "    \"nodes=196;unique_images=16\",\n",
    "    \"nodes=196;unique_images=64\",\n",
    "]\n",
    "\n",
    "logs = [\n",
    "    (\"central\", \"central;{variation}\"),\n",
    "    (\"community\", \"community12p;{variation}\"),\n",
    "    (\"community\", \"community25p;{variation}\"),\n",
    "    (\"p2p\", \"p2p;{variation}\"),\n",
    "    (\"dynamic\", \"p2p;{variation}\"),\n",
    "    (\"resource_aware_dynamic\", \"p2p;{variation}\", \"1\"),\n",
    "    (\"resource_aware_dynamic\", \"p2p;{variation}\", \"2\"),\n",
    "    (\"resource_aware_dynamic\", \"p2p;{variation}\", \"3\"),\n",
    "    (\"resource_aware_dynamic\", \"p2p;{variation}\", \"4\"),\n",
    "]\n",
    "\n",
    "logs_labels = [\"Central\", \"Comm*\", \"Comm+\", \"P2P\", \"LMDyn\", \"MODyn (1)\", \"MODyn (2)\", \"MODyn (3)\", \"MODyn (4)\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User data: latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_data(variation, user_type=None):\n",
    "    user_data = []\n",
    "\n",
    "    for index, log in enumerate(logs):\n",
    "        # Formatting log\n",
    "        log = list(log)\n",
    "        log[1] = log[1].format(variation=variation)\n",
    "\n",
    "        simulation_data = {\n",
    "            \"Algorithm\": logs_labels[index],\n",
    "            \"Mean Latency\": 0,\n",
    "        }\n",
    "\n",
    "        # Opening file\n",
    "        user_msgpack_file = (\n",
    "            f\"{base_dir}/algorithm={log[0]};dataset={log[1]};seed={seed}/User.msgpack\"\n",
    "            if len (log) == 2\n",
    "            else f\"{base_dir}/algorithm={log[0]};dataset={log[1]};seed={seed};replicas={log[2]}/User.msgpack\"\n",
    "        )\n",
    "        user_file = open(user_msgpack_file, \"rb\")\n",
    "        user_msgpack = msgpack.load(user_file)\n",
    "        user_df = pd.DataFrame(user_msgpack)\n",
    "\n",
    "        # Collecting information\n",
    "        latency = (\n",
    "            user_df[[\"Object\", \"Delays\"]]\n",
    "            if user_type is None\n",
    "            else user_df[user_df[\"User Type\"] == user_type][[\"Object\", \"Delays\"]]\n",
    "        )\n",
    "\n",
    "        # Persisting information\n",
    "        simulation_data[\"Mean Latency\"] = latency[\"Delays\"].mean()\n",
    "\n",
    "        # Closing file\n",
    "        user_file.close()\n",
    "\n",
    "        # Appending data\n",
    "        user_data.append(simulation_data)\n",
    "\n",
    "    return pd.DataFrame(user_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data_dfs = {\n",
    "    variation: get_user_data(variation)\n",
    "    for variation in variations\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registry data: registry usage and number of provisioned registries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_registry_data(variation):\n",
    "    registry_data = []\n",
    "    registry_data_per_time_step = {}\n",
    "\n",
    "    for index, log in enumerate(logs):\n",
    "        # Formatting log\n",
    "        log = list(log)\n",
    "        log[1] = log[1].format(variation=variation)\n",
    "\n",
    "        simulation_data = {\n",
    "            \"Algorithm\": logs_labels[index],\n",
    "            \"Values\": [],\n",
    "            \"Values (w/o 0%)\": [],\n",
    "            \"Mean\": 0,\n",
    "            \"Mean Provisioned\": 0,\n",
    "        }\n",
    "\n",
    "        # Opening file\n",
    "        registry_msgpack_file = (\n",
    "            f\"{base_dir}/algorithm={log[0]};dataset={log[1]};seed={seed}/ContainerRegistry.msgpack\"\n",
    "            if len (log) == 2\n",
    "            else f\"{base_dir}/algorithm={log[0]};dataset={log[1]};seed={seed};replicas={log[2]}/ContainerRegistry.msgpack\"\n",
    "        )\n",
    "        registry_file = open(registry_msgpack_file, \"rb\")\n",
    "        registry_msgpack = msgpack.load(registry_file)\n",
    "        registry_df = pd.DataFrame(registry_msgpack)\n",
    "\n",
    "        # Collecting information\n",
    "        registry_filtered_data = (\n",
    "            registry_df[registry_df[\"P2P\"] == True][[\"Object\", \"Provisioning\", \"Not Provisioning\"]]\n",
    "            if log[1] == \"p2p\"\n",
    "            else registry_df[[\"Object\", \"Provisioning\", \"Not Provisioning\"]]\n",
    "        )\n",
    "        registry_data_grouped = registry_filtered_data.groupby(\"Object\").sum()\n",
    "        registry_data_grouped[\"Total Steps\"] = registry_data_grouped.sum(axis=1)\n",
    "        registry_data_grouped[\"Provisioning Percentage\"] = registry_data_grouped[\"Provisioning\"] / registry_data_grouped[\"Total Steps\"]\n",
    "        registry_data_grouped[\"Not Provisioning Percentage\"] = registry_data_grouped[\"Not Provisioning\"] / registry_data_grouped[\"Total Steps\"]\n",
    "        registries_per_time_step = registry_df[registry_df[\"Time Step\"] > 0].groupby([\"Time Step\"]).count()[\"Object\"]\n",
    "\n",
    "        # Persisting information\n",
    "        simulation_data[\"Values\"] = registry_data_grouped[\"Provisioning Percentage\"]\n",
    "        simulation_data[\"Values (w/o 0%)\"] = registry_data_grouped[registry_data_grouped[\"Provisioning Percentage\"] > 0][\"Provisioning Percentage\"]\n",
    "        simulation_data[\"Mean\"] = registry_data_grouped[\"Provisioning Percentage\"].mean()\n",
    "        simulation_data[\"Mean Provisioned\"] = registry_data_grouped[\"Total Steps\"].sum() / last_step\n",
    "        registry_data_per_time_step[logs_labels[index]] = registries_per_time_step\n",
    "\n",
    "        # Closing file\n",
    "        registry_file.close()\n",
    "\n",
    "        # Appending data\n",
    "        registry_data.append(simulation_data)\n",
    "\n",
    "    return pd.DataFrame(registry_data), pd.concat(registry_data_per_time_step, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_data_dfs = {\n",
    "    variation: get_registry_data(variation)\n",
    "    for variation in variations\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge Server data: computing resources utilization and disk utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_cpu_and_memory(cpu, memory) -> float:\n",
    "    \"\"\"Normalizes the CPU and memory values.\n",
    "\n",
    "    Args:\n",
    "        cpu (float): CPU value.\n",
    "        memory (float): Memory value.\n",
    "\n",
    "    Returns:\n",
    "        normalized_value (float): Normalized value.\n",
    "    \"\"\"\n",
    "    normalized_value = (cpu * memory) ** (1 / 2)\n",
    "    return normalized_value\n",
    "    \n",
    "def get_server_data(variation):\n",
    "    computing_utilization_data = {}\n",
    "    disk_utilization_data = {}\n",
    "    server_data = []\n",
    "\n",
    "    for index, log in enumerate(logs):\n",
    "        # Formatting log\n",
    "        log = list(log)\n",
    "        log[1] = log[1].format(variation=variation)\n",
    "\n",
    "        simulation_data = {\n",
    "            \"Algorithm\": logs_labels[index],\n",
    "            \"Mean Disk Occupation per Server\": 0,\n",
    "        }\n",
    "        \n",
    "        # Opening file\n",
    "        server_msgpack_file = (\n",
    "            f\"{base_dir}/algorithm={log[0]};dataset={log[1]};seed={seed}/EdgeServer.msgpack\"\n",
    "            if len (log) == 2\n",
    "            else f\"{base_dir}/algorithm={log[0]};dataset={log[1]};seed={seed};replicas={log[2]}/EdgeServer.msgpack\"\n",
    "        )\n",
    "        server_file = open(server_msgpack_file, \"rb\")\n",
    "        server_msgpack = msgpack.load(server_file)\n",
    "        server_df = pd.DataFrame(server_msgpack)\n",
    "\n",
    "        # Collecting information\n",
    "        server_df = server_df[[\"Object\", \"CPU\", \"RAM\", \"CPU Demand\", \"RAM Demand\", \"Disk Demand\", \"Time Step\"]]\n",
    "        server_df[\"Normalized Utilization\"] = server_df.apply(lambda row: normalize_cpu_and_memory(row[\"CPU Demand\"], row[\"RAM Demand\"])/normalize_cpu_and_memory(row[\"CPU\"], row[\"RAM\"]), axis=1)\n",
    "        computing_utilization_per_time_step = server_df[server_df[\"Time Step\"] > 0].groupby([\"Time Step\"])[\"Normalized Utilization\"].mean()\n",
    "        disk_utilization_per_time_step = server_df[server_df[\"Time Step\"] > 0].groupby([\"Time Step\"])[\"Disk Demand\"].sum()\n",
    "        number_of_unique_servers = server_df[\"Object\"].nunique()\n",
    "\n",
    "        # Persisting information\n",
    "        computing_utilization_data[logs_labels[index]] = computing_utilization_per_time_step\n",
    "        disk_utilization_data[logs_labels[index]] = disk_utilization_per_time_step\n",
    "        simulation_data[\"Total Disk Occupation\"] = server_df[\"Disk Demand\"].sum()\n",
    "        simulation_data[\"Mean Disk Occupation per Server\"] = simulation_data[\"Total Disk Occupation\"] / (number_of_unique_servers * 3600)\n",
    "\n",
    "        # Closing file\n",
    "        server_file.close()\n",
    "\n",
    "        # Appending data\n",
    "        server_data.append(simulation_data)\n",
    "\n",
    "    return pd.concat(computing_utilization_data, axis=1), pd.concat(disk_utilization_data, axis=1), pd.DataFrame(server_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_data_dfs = {\n",
    "    variation: get_server_data(variation)\n",
    "    for variation in variations\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application data: provisioning time and reallocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_application_data(variation):\n",
    "    service_data = []\n",
    "\n",
    "    for index, log in enumerate(logs):\n",
    "        # Formatting log\n",
    "        log = list(log)\n",
    "        log[1] = log[1].format(variation=variation)\n",
    "\n",
    "        simulation_data = {\n",
    "            \"Algorithm\": logs_labels[index],\n",
    "            \"Prov. Time Values\": [],\n",
    "            \"Mean Prov. Time\": 0,\n",
    "            \"Only Using Cache\": 0,\n",
    "            \"Partially Using Cache\": 0,\n",
    "            \"Not Using Cache\": 0,\n",
    "            \"Total Migrations\": 0,\n",
    "            \"Only Using Cache (%)\": 0,\n",
    "            \"Partially Using Cache (%)\": 0,\n",
    "            \"Not Using Cache (%)\": 0,\n",
    "        }\n",
    "\n",
    "        # Opening file\n",
    "        service_msgpack_file = (\n",
    "            f\"{base_dir}/algorithm={log[0]};dataset={log[1]};seed={seed}/Service.msgpack\"\n",
    "            if len (log) == 2\n",
    "            else f\"{base_dir}/algorithm={log[0]};dataset={log[1]};seed={seed};replicas={log[2]}/Service.msgpack\"\n",
    "        )\n",
    "        service_file = open(service_msgpack_file, \"rb\")\n",
    "        service_msgpack = msgpack.load(service_file)\n",
    "        service_df = pd.DataFrame(service_msgpack)\n",
    "\n",
    "        # Collecting information\n",
    "        migrations_duration = service_df[service_df[\"Time Step\"] == last_step][\"Migrations Duration\"].apply(pd.Series).stack().reset_index(drop=True)\n",
    "        migrations_last_step = service_df[service_df[\"Time Step\"] == last_step][[\"Object\", \"Migrations (Only Cache)\", \"Migrations (Partial Cache)\", \"Migrations (No Cache)\"]]\n",
    "\n",
    "        # Persisting information\n",
    "        simulation_data[\"Prov. Time Values\"] = service_df[service_df[\"Time Step\"] == last_step][\"Migrations Duration\"].apply(pd.Series).stack().reset_index(drop=True)\n",
    "        simulation_data[\"Mean Prov. Time\"] = migrations_duration.mean()\n",
    "        simulation_data[\"Only Using Cache\"] = migrations_last_step[\"Migrations (Only Cache)\"].sum()\n",
    "        simulation_data[\"Partially Using Cache\"] = migrations_last_step[\"Migrations (Partial Cache)\"].sum()\n",
    "        simulation_data[\"Not Using Cache\"] = migrations_last_step[\"Migrations (No Cache)\"].sum()\n",
    "        simulation_data[\"Total Migrations\"] = simulation_data[\"Only Using Cache\"] + simulation_data[\"Partially Using Cache\"] + simulation_data[\"Not Using Cache\"]\n",
    "        simulation_data[\"Only Using Cache (%)\"] = simulation_data[\"Only Using Cache\"] / simulation_data[\"Total Migrations\"]\n",
    "        simulation_data[\"Partially Using Cache (%)\"] = simulation_data[\"Partially Using Cache\"] / simulation_data[\"Total Migrations\"]\n",
    "        simulation_data[\"Not Using Cache (%)\"] = simulation_data[\"Not Using Cache\"] / simulation_data[\"Total Migrations\"]\n",
    "\n",
    "        # Closing file\n",
    "        service_file.close()\n",
    "\n",
    "        # Appending data\n",
    "        service_data.append(simulation_data)\n",
    "\n",
    "    return pd.DataFrame(service_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_data_dfs = {\n",
    "    variation: get_application_data(variation)\n",
    "    for variation in variations\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topology data: image replication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_replication_data(log, variation):\n",
    "    # Formatting log\n",
    "    log = list(log)\n",
    "    log[1] = log[1].format(variation=variation)\n",
    "    \n",
    "    # Opening file\n",
    "    topology_msgpack_file = (\n",
    "        f\"{base_dir}/algorithm={log[0]};dataset={log[1]};seed={seed}/Topology.msgpack\"\n",
    "        if len (log) == 2\n",
    "        else f\"{base_dir}/algorithm={log[0]};dataset={log[1]};seed={seed};replicas={log[2]}/Topology.msgpack\"\n",
    "    )\n",
    "    topology_file = open(topology_msgpack_file, \"rb\")\n",
    "    topology_msgpack = msgpack.load(topology_file, strict_map_key=False)\n",
    "    topology_df = pd.DataFrame(topology_msgpack)\n",
    "\n",
    "    # Collecting information\n",
    "    image_replication_data = topology_df[[\"Object\", \"Replication Data\", \"Time Step\"]]\n",
    "    image_replication_data = pd.json_normalize(image_replication_data[\"Replication Data\"]).fillna(0)\n",
    "    image_replication_data = image_replication_data.divide(image_replication_data.sum(axis=1), axis=0)\n",
    "\n",
    "    # Sort headers\n",
    "    image_replication_data = image_replication_data.reindex(sorted(image_replication_data.columns), axis=1)\n",
    "\n",
    "    # Closing file\n",
    "    topology_file.close()\n",
    "\n",
    "    return image_replication_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilitary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customize_chart(\n",
    "    ax,\n",
    "    title: str = \"\",\n",
    "    xlabel: str = \"\",\n",
    "    ylabel: str = \"\",\n",
    "    xticklabels: list = [],\n",
    "    yticklabels: list = [],\n",
    "    legend: dict = {},\n",
    "):\n",
    "    \"\"\"Customizes the chart.\n",
    "    \n",
    "    Args:\n",
    "        ax (matplotlib.axes.Axes): Axes object.\n",
    "        title (str): Chart title.\n",
    "        xticklabels (list): List of xtick labels.\n",
    "        xlabel (str): Label of the x axis.\n",
    "        yticklabels (list): List of ytick labels.\n",
    "        ylabel (str): Label of the y axis.\n",
    "        legend_labels (list): List of legend labels.\n",
    "    \"\"\"\n",
    "    if title != \"\":\n",
    "        ax.set_title(title)\n",
    "\n",
    "    if xlabel != \"\":\n",
    "        ax.set_xlabel(xlabel)\n",
    "\n",
    "    if ylabel != \"\":\n",
    "        ax.set_ylabel(ylabel)\n",
    "\n",
    "    if xticklabels != []:\n",
    "        ax.set_xticklabels(xticklabels)\n",
    "    \n",
    "    if yticklabels != []:\n",
    "        ax.set_yticklabels(yticklabels)\n",
    "\n",
    "    if legend != {}:\n",
    "        ax.legend(**legend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Container images size distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading container images metadata\n",
    "container_images = json.load(open(\"datasets/inputs/templates/container_images.json\", \"r\"))\n",
    "\n",
    "# Ignoring registry image\n",
    "container_images = container_images[1:]\n",
    "\n",
    "# Collecting size of images\n",
    "image_sizes = [sum([layer[\"size\"] for layer in image[\"layers\"]])/1000000 for image in container_images]\n",
    "\n",
    "# Plot size distribution\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist(image_sizes, bins=10, edgecolor=\"black\", color=\"white\")\n",
    "\n",
    "# Customizing chart\n",
    "customize_chart(\n",
    "    ax,\n",
    "    xlabel=\"Size (MiB)\",\n",
    "    ylabel=\"Number of container images\",\n",
    ")\n",
    "\n",
    "# Saving figure\n",
    "plt.savefig(f\"{base_dir}/image-size-distribution.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Container layers sharing information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_layers = [layer for image in container_images for layer in image[\"layers\"]]\n",
    "\n",
    "layers_metadata = {}\n",
    "for layer in container_layers:\n",
    "    if layer[\"digest\"] not in layers_metadata:\n",
    "        layers_metadata[layer[\"digest\"]] = {\"size\": layer[\"size\"], \"images_sharing\": 1}\n",
    "    else:\n",
    "        layers_metadata[layer[\"digest\"]][\"images_sharing\"] += 1\n",
    "\n",
    "sharing_info = {}\n",
    "for layer_metadata in layers_metadata.values():\n",
    "    if layer_metadata[\"images_sharing\"] not in sharing_info:\n",
    "        sharing_info[layer_metadata[\"images_sharing\"]] = { \"total_size\": layer_metadata[\"size\"] }\n",
    "    else:\n",
    "        sharing_info[layer_metadata[\"images_sharing\"]][\"total_size\"] += layer_metadata[\"size\"]\n",
    "\n",
    "total_size = sum([layer_metadata[\"size\"] for layer_metadata in layers_metadata.values()])\n",
    "for key in sharing_info:\n",
    "    sharing_info[key][\"percentage\"] = round(sharing_info[key][\"total_size\"] / total_size * 100, 3)\n",
    "    sharing_info[key][\"total_size\"] = round(sharing_info[key][\"total_size\"] / 1000000, 3)\n",
    "\n",
    "layer_sharing_df = pd.DataFrame(sharing_info).T.sort_index()\n",
    "layer_sharing_df.reset_index(inplace=True)\n",
    "layer_sharing_df.columns = [\"Number of images sharing\", \"Total size (MiB)\", \"Percentage of total size (%)\"]\n",
    "layer_sharing_df[\"Number of images sharing\"] = layer_sharing_df[\"Number of images sharing\"].astype(str)\n",
    "layer_sharing_df = pd.concat([layer_sharing_df, pd.DataFrame({\"Number of images sharing\": \"Total\", \"Total size (MiB)\": layer_sharing_df[\"Total size (MiB)\"].sum(), \"Percentage of total size (%)\": 100}, index=[0])])\n",
    "\n",
    "layer_sharing_latex_string = layer_sharing_df.to_latex(\n",
    "    float_format=\"%.3f\",\n",
    "    column_format=\"lcc\",\n",
    "    escape=True,\n",
    "    caption=\"AWS Deep Learning layer-sharing information.\",\n",
    "    label=\"tab:layer-sharing\",\n",
    "    index=False\n",
    ").replace(\n",
    "    \"\\\\begin{table}\", \"\\\\begin{table}\\n\\\\centering\"\n",
    ").replace(\n",
    "    \"Total &\", \"\\midrule\\nTotal &\"\n",
    ")\n",
    "\n",
    "with open(\"logs/layer_sharing.tex\", \"w\") as file:\n",
    "    file.write(layer_sharing_latex_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_datasets(target_variation: str):\n",
    "    datasets = [\n",
    "        \"central\",\n",
    "        \"community12p\",\n",
    "        \"community25p\",\n",
    "        \"p2p\",\n",
    "    ]\n",
    "\n",
    "    dataset_labels = [\n",
    "        \"Central\",\n",
    "        \"Community (12%)\",\n",
    "        \"Community (25%)\",\n",
    "        \"P2P\",\n",
    "    ]\n",
    "\n",
    "    base_station_hatch = \"\"\n",
    "    edge_server_hatch = \"////\"\n",
    "    p2p_registry_hatch = \"xxxx\"\n",
    "    full_registry_hatch = \"**\"\n",
    "\n",
    "    plt.rcParams.update({'axes.titlesize': 18, 'legend.fontsize': 18})\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    for index, dataset in enumerate(datasets):\n",
    "        simulator = edge_sim_py.Simulator()\n",
    "        simulator.initialize(input_file=f\"datasets/{dataset};{target_variation}.json\")\n",
    "        topology = edge_sim_py.Topology.first()\n",
    "        positions = {\n",
    "            node: node.coordinates\n",
    "            for node in topology.nodes()\n",
    "        }\n",
    "\n",
    "        # Gathering the coordinates of edge server\n",
    "        edge_server_coordinates = [edge_server.coordinates for edge_server in edge_sim_py.EdgeServer.all()]\n",
    "\n",
    "        # Gathering the coordinates of container registries\n",
    "        full_registry_coordinates = [\n",
    "            edge_server.coordinates\n",
    "            for edge_server\n",
    "            in edge_sim_py.EdgeServer.all()\n",
    "            if len([registry for registry in edge_server.container_registries if registry.available and not registry.p2p_registry]) > 0\n",
    "        ]\n",
    "        p2p_registry_coordinates = [\n",
    "            edge_server.coordinates\n",
    "            for edge_server\n",
    "            in edge_sim_py.EdgeServer.all()\n",
    "            if len([registry for registry in edge_server.container_registries if registry.available and registry.p2p_registry]) > 0\n",
    "        ]\n",
    "\n",
    "        nx.draw_networkx_edges(topology, positions, ax=ax[index // 2][index % 2], edge_color=\"black\", width=1)\n",
    "\n",
    "        for node in topology.nodes():\n",
    "            coordinates = node.coordinates\n",
    "            hatch = full_registry_hatch if coordinates in full_registry_coordinates else p2p_registry_hatch if coordinates in p2p_registry_coordinates else edge_server_hatch if coordinates in edge_server_coordinates else base_station_hatch\n",
    "            ax[index // 2][index % 2].scatter(\n",
    "                coordinates[0],\n",
    "                coordinates[1],\n",
    "                s=250,\n",
    "                c=\"white\",\n",
    "                marker=\"o\",\n",
    "                edgecolor=\"black\",\n",
    "                linewidth=0.5,\n",
    "                hatch=hatch,\n",
    "            )\n",
    "\n",
    "        ax[index // 2][index % 2].set_title(dataset_labels[index])\n",
    "        ax[index // 2][index % 2].axis(\"off\")\n",
    "        \n",
    "    # add legend with custom hatches\n",
    "    fig.legend(\n",
    "        [\n",
    "            mpl.patches.Patch(hatch=base_station_hatch, facecolor=\"white\", edgecolor=\"black\"),\n",
    "            mpl.patches.Patch(hatch=edge_server_hatch, facecolor=\"white\", edgecolor=\"black\"),\n",
    "            mpl.patches.Patch(hatch=p2p_registry_hatch, facecolor=\"white\", edgecolor=\"black\"),\n",
    "            mpl.patches.Patch(hatch=full_registry_hatch, facecolor=\"white\", edgecolor=\"black\"),\n",
    "        ],\n",
    "        [\n",
    "            \"Only base station (BS)\",\n",
    "            \"BS + edge server (ES)\",\n",
    "            \"BS + ES + P2P registry\",\n",
    "            \"BS + ES + fully replicated registry\",\n",
    "\n",
    "        ],\n",
    "        loc=\"center\",\n",
    "        bbox_to_anchor=(0.5, 0.06),\n",
    "        ncol=2,\n",
    "    )\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.0, hspace=0.075)\n",
    "    plt.savefig(f\"{base_dir}/datasets-{target_variation}.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_datasets(\"nodes=100;unique_images=08\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_datasets(\"nodes=196;unique_images=64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latency: comparative table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latency_table = pd.DataFrame(\n",
    "    {\n",
    "        variation: user_data_dfs[variation][\"Mean Latency\"]\n",
    "        for variation in variations\n",
    "    }\n",
    ")\n",
    "\n",
    "latency_table.index = logs_labels\n",
    "\n",
    "latency_table_latex_string = latency_table.to_latex(\n",
    "    index=True,\n",
    "    escape=True,\n",
    "    header=False,\n",
    "    label=\"tab:latency\",\n",
    "    column_format=\"lcccc\",\n",
    "    caption=\"Overall mean latency in time units (Equation~\\\\ref{eq:minimize-latency}).\",\n",
    "    float_format=\"{:0.2f}\".format\n",
    ").replace(\n",
    "    \"\\\\begin{table}\",\n",
    "    \"\\\\begin{table}\\n\\centering\"\n",
    ").replace(\n",
    "    \"\\\\toprule\",\n",
    "    \"\\\\toprule\\nNodes & \\multicolumn{2}{c}{100} & \\multicolumn{2}{c}{196} \\\\\\\\\\n\\midrule\\nUnique images & 08 & 32 & 16 & 64 \\\\\\\\\"\n",
    ")\n",
    "\n",
    "with open(f\"{base_dir}/table_latency.tex\", \"w\") as f:\n",
    "    f.write(latency_table_latex_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resource usage (provisioned registries): comparative table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provisioned_registries_table = pd.DataFrame(\n",
    "    {\n",
    "        variation: registry_data_dfs[variation][0][\"Mean Provisioned\"]\n",
    "        for variation in variations\n",
    "    }\n",
    ")\n",
    "\n",
    "provisioned_registries_table.index = logs_labels\n",
    "\n",
    "provisioned_registries_table_latex_string = provisioned_registries_table.to_latex(\n",
    "    index=True,\n",
    "    escape=True,\n",
    "    header=False,\n",
    "    label=\"tab:provisioned-registries\",\n",
    "    column_format=\"lcccc\",\n",
    "    caption=\"Mean number of provisioned registries per time step (Equation~\\\\ref{eq:minimize-computing}).\",\n",
    "    float_format=\"{:0.2f}\".format\n",
    ").replace(\n",
    "    \"\\\\begin{table}\",\n",
    "    \"\\\\begin{table}\\n\\centering\"\n",
    ").replace(\n",
    "    \"\\\\toprule\",\n",
    "    \"\\\\toprule\\nNodes & \\multicolumn{2}{c}{100} & \\multicolumn{2}{c}{196} \\\\\\\\\\n\\midrule\\nUnique images & 08 & 32 & 16 & 64 \\\\\\\\\"\n",
    ")\n",
    "\n",
    "with open(f\"{base_dir}/table_provisioned_registries.tex\", \"w\") as f:\n",
    "    f.write(provisioned_registries_table_latex_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resource usage (provisioned registries along the time): line chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 14, 'axes.titlesize': 14, 'legend.fontsize': 12})\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True)\n",
    "\n",
    "for counter, (key, registry_data) in enumerate(registry_data_dfs.items()):\n",
    "    registry_data[1].plot(ax=ax[counter // 2][counter % 2], legend=False, xticks=range(0, last_step + 1, 1200), yticks=range(0, 51, 10))\n",
    "    customize_chart(\n",
    "        ax[counter // 2][counter % 2],\n",
    "        title=key,\n",
    "    )\n",
    "    ax[counter // 2][counter % 2].set_xlabel(\"\")\n",
    "\n",
    "handles, _ = plt.gca().get_legend_handles_labels()\n",
    "fig.legend(handles, logs_labels, loc='center', ncols=5, bbox_to_anchor=(0.5, 0.94))\n",
    "fig.text(0.07, 0.5, \"Number of provisioned registries\", va='center', rotation='vertical')\n",
    "fig.text(0.5, 0.06, \"Time step\", ha='center')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.125)\n",
    "plt.savefig(f\"{base_dir}/number-of-provisioned-registries.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resource usage (server computing utilization along the time): line chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 14, 'axes.titlesize': 14, 'legend.fontsize': 12})\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True)\n",
    "\n",
    "for counter, (key, server_data) in enumerate(server_data_dfs.items()):\n",
    "    server_data[0].plot(ax=ax[counter // 2][counter % 2], legend=False, xticks=range(0, last_step + 1, 1200), yticks=[0.58, 0.6, 0.62, 0.64, 0.66, 0.68])\n",
    "    customize_chart(\n",
    "        ax[counter // 2][counter % 2],\n",
    "        title=key,\n",
    "    )\n",
    "    ax[counter // 2][counter % 2].yaxis.set_major_formatter(mticker.PercentFormatter(xmax=1, decimals=0))\n",
    "    ax[counter // 2][counter % 2].set_xlabel(\"\")\n",
    "\n",
    "handles, _ = plt.gca().get_legend_handles_labels()\n",
    "fig.legend(handles, logs_labels, loc='center', ncols=5, bbox_to_anchor=(0.5, 0.94))\n",
    "fig.text(0.05, 0.5, \"Normalized server utilization\", va='center', rotation='vertical')\n",
    "fig.text(0.5, 0.06, \"Time step\", ha='center')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.125)\n",
    "plt.savefig(f\"{base_dir}/normalized-mean-server-utilization-computing.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provisioning time: comparative table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provisioning_time_table = pd.DataFrame(\n",
    "    {\n",
    "        variation: application_data_dfs[variation][\"Mean Prov. Time\"]\n",
    "        for variation in variations\n",
    "    }\n",
    ")\n",
    "\n",
    "provisioning_time_table.index = logs_labels\n",
    "\n",
    "provisioning_time_table_latex_string = provisioning_time_table.to_latex(\n",
    "    index=True,\n",
    "    escape=True,\n",
    "    header=False,\n",
    "    label=\"tab:provisioning-time\",\n",
    "    column_format=\"lcccc\",\n",
    "    caption=\"Overall mean provisioning time in seconds (Equation~\\\\ref{eq:minimize-provisioning-time}).\",\n",
    "    float_format=\"{:0.2f}\".format\n",
    ").replace(\n",
    "    \"\\\\begin{table}\",\n",
    "    \"\\\\begin{table}\\n\\centering\"\n",
    ").replace(\n",
    "    \"\\\\toprule\",\n",
    "    \"\\\\toprule\\nNodes & \\multicolumn{2}{c}{100} & \\multicolumn{2}{c}{196} \\\\\\\\\\n\\midrule\\nUnique images & 08 & 32 & 16 & 64 \\\\\\\\\"\n",
    ")\n",
    "\n",
    "with open(f\"{base_dir}/table_provisioning_time.tex\", \"w\") as f:\n",
    "    f.write(provisioning_time_table_latex_string)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reallocations per type: stacker bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 14, 'axes.titlesize': 14, 'legend.fontsize': 12})\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True)\n",
    "\n",
    "for counter, (key, application_data) in enumerate(application_data_dfs.items()):\n",
    "    reallocations_df = application_data[[\"Algorithm\", \"Only Using Cache\", \"Partially Using Cache\", \"Not Using Cache\"]]\n",
    "    reallocations_df.plot.bar(x=\"Algorithm\", stacked=True, ax=ax[counter // 2][counter % 2], legend=False)\n",
    "    customize_chart(\n",
    "        ax[counter // 2][counter % 2],\n",
    "        title=key,\n",
    "    )\n",
    "    ax[counter // 2][counter % 2].set_xlabel(\"\")\n",
    "    ax[counter // 2][counter % 2].set_xticklabels(logs_labels, rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "handles, _ = plt.gca().get_legend_handles_labels()\n",
    "labels = [\"Only using cache\", \"Partially using cache\", \"Not using cache\"]\n",
    "fig.legend(handles, labels, loc='center', ncols=3, bbox_to_anchor=(0.5, 0.93))\n",
    "fig.text(0.04, 0.5, \"Number of reallocations\", va='center', rotation='vertical')\n",
    "fig.text(0.5, 0, \"Algorithm\", ha='center')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.125)\n",
    "plt.savefig(f\"{base_dir}/reallocations-per-type.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disk utilization: comparative table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disk_utilization_table = pd.DataFrame(\n",
    "    {\n",
    "        variation: server_data_dfs[variation][2][\"Mean Disk Occupation per Server\"]\n",
    "        for variation in variations\n",
    "    }\n",
    ")\n",
    "\n",
    "disk_utilization_table.index = logs_labels\n",
    "\n",
    "disk_utilization_table_latex_string = disk_utilization_table.to_latex(\n",
    "    index=True,\n",
    "    escape=True,\n",
    "    header=False,\n",
    "    label=\"tab:disk-utilization\",\n",
    "    column_format=\"lcccc\",\n",
    "    caption=\"Mean disk utilization per edge server and time step in MiB (Equation~\\\\ref{eq:minimize-storage}).\",\n",
    "    float_format=\"{:0.2f}\".format\n",
    ").replace(\n",
    "    \"\\\\begin{table}\",\n",
    "    \"\\\\begin{table}\\n\\centering\"\n",
    ").replace(\n",
    "    \"\\\\toprule\",\n",
    "    \"\\\\toprule\\nNodes & \\multicolumn{2}{c}{100} & \\multicolumn{2}{c}{196} \\\\\\\\\\n\\midrule\\nUnique images & 08 & 32 & 16 & 64 \\\\\\\\\"\n",
    ")\n",
    "\n",
    "with open(f\"{base_dir}/table_disk_utilization.tex\", \"w\") as f:\n",
    "    f.write(disk_utilization_table_latex_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disk utilization along the time: line chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sizeof_fmt(x, pos):\n",
    "    if x < 0:\n",
    "        return \"\"\n",
    "    for x_unit in ['MB', 'GB', 'TB']:\n",
    "        if x < 1024.0:\n",
    "            return \"%3.1f %s\" % (x, x_unit)\n",
    "        x /= 1024.0\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True)\n",
    "\n",
    "for counter, (key, server_data) in enumerate(server_data_dfs.items()):\n",
    "    server_data[1].plot(ax=ax[counter // 2][counter % 2], legend=False, xticks=range(0, last_step + 1, 1200))\n",
    "    customize_chart(\n",
    "        ax[counter // 2][counter % 2],\n",
    "        title=key,\n",
    "    )\n",
    "    ax[counter // 2][counter % 2].yaxis.set_major_formatter(mticker.FuncFormatter(sizeof_fmt))\n",
    "    ax[counter // 2][counter % 2].set_xlabel(\"\")\n",
    "\n",
    "handles, _ = plt.gca().get_legend_handles_labels()\n",
    "fig.legend(handles, logs_labels, loc='center', ncols=5, bbox_to_anchor=(0.5, 0.94))\n",
    "fig.text(0, 0.5, \"Total disk utilization\", va='center', rotation='vertical')\n",
    "fig.text(0.5, 0.06, \"Time step\", ha='center')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.125)\n",
    "plt.savefig(f\"{base_dir}/total-disk-utilization.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registry usage (percentage of active steps): boxplot chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check how to share labels\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True)\n",
    "\n",
    "for counter, (key, registry_data) in enumerate(registry_data_dfs.items()):\n",
    "    registry_usage_df = registry_data[0][[\"Algorithm\", \"Values\"]]\n",
    "    ax[counter // 2][counter % 2].boxplot(registry_usage_df[\"Values\"], labels=logs_labels)\n",
    "    customize_chart(\n",
    "        ax[counter // 2][counter % 2],\n",
    "        title=key,\n",
    "    )\n",
    "    ax[counter // 2][counter % 2].yaxis.set_major_formatter(mticker.PercentFormatter(xmax=1, decimals=0))\n",
    "\n",
    "fig.text(0.05, 0.5, \"Percentage of time active\", va='center', rotation='vertical')\n",
    "fig.text(0.5, 0, \"Algorithm\", ha='center')\n",
    "\n",
    "for counter in range(2):\n",
    "    ax[1][counter].set_xticklabels(ax[1][counter].get_xticklabels(), rotation=45, fontstyle=\"normal\", ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.125)\n",
    "plt.savefig(f\"{base_dir}/registries-percentage-of-time-active.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image replication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variation = \"nodes=100;unique_images=32\"\n",
    "\n",
    "image_replication_data = [\n",
    "    get_image_replication_data(log, target_variation) for log in logs[3:]\n",
    "]\n",
    "\n",
    "headers = [\n",
    "    image_replication_data[index].columns.values.tolist() for index in range(len(image_replication_data))\n",
    "]\n",
    "\n",
    "max_length = max([len(header) for header in headers])\n",
    "header_with_max_length = headers[[len(header) for header in headers].index(max_length)]\n",
    "\n",
    "global_colormap = mpl.colormaps[\"tab20c\"].resampled(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fix when it does not start with zero\n",
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot stacked area chart for each algorithm in multiple figures using a single legend\n",
    "plt.rcParams.update({'font.size': 14, 'axes.titlesize': 14, 'legend.fontsize': 12})\n",
    "fig, ax = plt.subplots(2, 3, figsize=(20, 10), sharex=True, sharey=True)\n",
    "\n",
    "for counter, image_replication_df in enumerate(image_replication_data):\n",
    "    local_color_map = mpl.colors.ListedColormap(global_colormap.colors[:len(image_replication_df.columns)])\n",
    "    image_replication_df.plot.area(ax=ax[counter // 3][counter % 3], legend=False, alpha=0.75, cmap=local_color_map, xticks=range(0, last_step + 1, 1200))\n",
    "    ax[counter // 3][counter % 3].set_ylim([0, 1])\n",
    "    ax[counter // 3][counter % 3].invert_yaxis()\n",
    "    customize_chart(\n",
    "        ax[counter // 3][counter % 3],\n",
    "        title=logs_labels[counter+3],\n",
    "    )\n",
    "\n",
    "# percentage formatter\n",
    "ax[0][0].yaxis.set_major_formatter(mticker.PercentFormatter(xmax=1, decimals=0))\n",
    "\n",
    "# create a single colorbar for all figures\n",
    "cax = fig.add_axes([0.91, 0.125, 0.02, 0.75])\n",
    "fig.colorbar(mpl.cm.ScalarMappable(cmap=global_colormap), cax=cax, orientation=\"vertical\", aspect=50, values=header_with_max_length, pad=0.1)\n",
    "fig.text(0.96, 0.5, \"Number of container image replicas\", va='center', rotation='vertical')\n",
    "\n",
    "fig.text(0.08, 0.5, \"Percentage of images with number of replicas\", va='center', rotation='vertical')\n",
    "fig.text(0.5, 0.06, \"Time step\", ha='center')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.125)\n",
    "plt.savefig(f\"{base_dir}/image-replication-{target_variation}.pdf\", bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
